# Claude Memory Intelligence - Installer Guide

**Version**: 1.0
**Last Updated**: October 5, 2025
**Status**: Production Ready

---

## üìã Overview

This guide covers the installation and usage of the Claude Memory Intelligence Ollama installer scripts. The installers provide a foolproof, automated way to set up Ollama with the `qwen2.5-coder:1.5b` model for enhanced intent analysis.

**Key Features**:
- ‚úÖ Automatic OS detection (macOS, Linux, WSL)
- ‚úÖ System requirement validation
- ‚úÖ One-command installation
- ‚úÖ Graceful fallback to regex-only mode
- ‚úÖ Idempotent (safe to run multiple times)
- ‚úÖ Comprehensive error handling

---

## üöÄ Quick Start

### Option 1: Bash Installer (Recommended for macOS/Linux)

```bash
# Navigate to project directory
cd ~/Projects/claude-memory-intelligence

# Run installer
./install.sh
```

### Option 2: Node.js Installer (Cross-platform)

```bash
# Navigate to project directory
cd ~/Projects/claude-memory-intelligence

# Run installer
node install.js
```

Both installers provide the same functionality. Choose based on your preference and environment.

---

## üì¶ What Gets Installed

### Full Installation (LLM Mode)

When everything succeeds, you'll have:

1. **Ollama Runtime** (~100MB)
   - Local LLM server running on `http://localhost:11434`
   - Auto-starts on system boot (on supported systems)

2. **qwen2.5-coder:1.5b Model** (~1GB)
   - 4-bit quantized model optimized for code
   - Cached locally for instant access

3. **Memory System Configuration**
   - Hybrid mode (LLM + Regex fallback)
   - Config file: `.memories/.llm-config.json`
   - Intent analysis accuracy: ~94.8%

### Fallback Installation (Regex-Only Mode)

If Ollama installation fails or is declined:

1. **Memory System Configuration**
   - Regex-only mode
   - Config file: `.memories/.llm-config.json`
   - Intent analysis accuracy: ~70%

2. **No Dependencies**
   - Pure JavaScript pattern matching
   - Works everywhere, no installation needed

---

## üîß Installation Options

### Interactive Installation (Default)

```bash
# Bash
./install.sh

# Node.js
node install.js
```

The installer will:
- Check system requirements
- Prompt for confirmation before installing Ollama
- Show progress for model download
- Configure memory system automatically

**User Prompts**:
1. "Continue?" - After system requirements check (if warnings)
2. "Install Ollama now?" - If Ollama not found
3. "Continue?" - Before downloading Ollama

### Dry-Run Mode (Test Without Changes)

```bash
# Bash
./install.sh --dry-run

# Node.js
node install.js --dry-run
```

Simulates installation without:
- Installing Ollama
- Downloading models
- Creating config files
- Making any system changes

**Use Cases**:
- Verify installer works on your system
- Check what will be installed
- Test error handling
- Preview installation steps

### Help Information

```bash
# Bash
./install.sh --help

# Node.js
node install.js --help
```

Shows usage information, options, and examples.

---

## üìä System Requirements

### Minimum Requirements

| Component | Minimum | Recommended | Critical? |
|-----------|---------|-------------|-----------|
| **RAM** | 8GB | 16GB | ‚ö†Ô∏è  YES (warns if <8GB) |
| **Disk Space** | 12GB | 50GB | ‚ö†Ô∏è  YES (warns if <12GB) |
| **Storage Type** | HDD | SSD | ‚ö° NO (but strongly recommended) |
| **Network** | Broadband | - | ‚úÖ YES (for initial download) |
| **OS** | macOS 10.15+ / Linux (kernel 4.0+) / WSL2 | - | ‚úÖ YES |
| **Architecture** | x86_64 / ARM64 | - | ‚úÖ YES |

### System Requirement Warnings

If your system doesn't meet minimum requirements, the installer will:

1. **Show Warning**:
   ```
   ‚ö†Ô∏è  System Warnings:
     ‚ö†Ô∏è  RAM: 6GB detected, 8GB recommended
     ‚ö†Ô∏è  Disk: 10GB free, 12GB recommended
   ```

2. **Explain Impact**:
   - Slower model loading times
   - Potential memory swapping
   - Longer response times

3. **Ask for Confirmation**:
   ```
   Continue anyway? (y/N):
   ```

4. **Proceed or Exit** based on your choice

---

## üéØ Installation Phases

The installer runs through 5 phases:

### Phase 1: Environment Detection

**What Happens**:
- Detects OS (macOS, Linux, WSL, Windows)
- Detects architecture (x86_64, ARM64)
- Checks RAM and disk space

**Output Example**:
```
‚ñ∂ Phase 1/5: Detecting environment...
‚ÑπÔ∏è  Detected: macos_arm64
‚úÖ System requirements met (24GB RAM, 87GB free)
```

### Phase 2: Ollama Installation Check

**What Happens**:
- Checks if Ollama already installed
- If not found, prompts for installation
- Downloads and runs official install script
- Verifies installation success

**Output Example**:
```
‚ñ∂ Phase 2/5: Checking Ollama installation...
‚úÖ Ollama already installed: ollama version is 0.12.3
```

### Phase 3: Server & Model Setup

**What Happens**:
- Starts Ollama server (if not running)
- Checks if model already downloaded
- Pulls `qwen2.5-coder:1.5b` model (~1GB)
- Waits for completion with progress

**Output Example**:
```
‚ñ∂ Phase 3/5: Setting up Ollama server and model...
‚ÑπÔ∏è  Ollama server already running
‚úÖ Model already downloaded: qwen2.5-coder:1.5b
```

### Phase 4: Memory System Configuration

**What Happens**:
- Creates `.memories/` directory (if needed)
- Writes `.llm-config.json` configuration
- Sets mode: `hybrid` or `regex-only`

**Output Example**:
```
‚ñ∂ Phase 4/5: Configuring memory system...
‚úÖ Memory system configured for LLM mode
‚ÑπÔ∏è  Config: .memories/.llm-config.json
```

### Phase 5: Final Verification

**What Happens**:
- Verifies all components installed
- Tests model (optional)
- Shows final status message

**Output Example**:
```
‚ñ∂ Phase 5/5: Verifying installation...
‚úÖ INSTALLATION COMPLETE

‚ÑπÔ∏è  Memory System Configuration:
  Mode:     Hybrid (LLM + Regex)
  Model:    qwen2.5-coder:1.5b
  Endpoint: http://localhost:11434
  Fallback: Regex patterns enabled
```

---

## ‚úÖ Success Scenarios

### Scenario 1: Full Installation (LLM Mode)

**Conditions**:
- Ollama installed successfully
- Server started
- Model downloaded
- Configuration created

**Result**:
```
‚úÖ INSTALLATION COMPLETE

Memory System Configuration:
  Mode:     Hybrid (LLM + Regex)
  Model:    qwen2.5-coder:1.5b
  Endpoint: http://localhost:11434
  Fallback: Regex patterns enabled

You can now use enhanced intent analysis!
```

**Files Created**:
```
.memories/
‚îî‚îÄ‚îÄ .llm-config.json    # Hybrid mode configuration
```

**What Works**:
- ‚úÖ LLM-based intent analysis (~94.8% accuracy)
- ‚úÖ Automatic fallback to regex if LLM fails
- ‚úÖ Enhanced entity extraction
- ‚úÖ Context-aware decision queries

### Scenario 2: Fallback Installation (Regex-Only Mode)

**Conditions**:
- Ollama installation declined
- OR installation failed
- OR model download failed

**Result**:
```
‚ö†Ô∏è  INSTALLATION COMPLETE (FALLBACK MODE)

Memory System Configuration:
  Mode:     Regex-only
  Reason:   Ollama installation declined/failed
  Features: Basic pattern matching

Current Capabilities:
  ‚úÖ Keyword-based intent detection
  ‚úÖ Simple entity extraction
  ‚ö†Ô∏è  Lower accuracy (~70% vs 94.8%)
```

**Files Created**:
```
.memories/
‚îî‚îÄ‚îÄ .llm-config.json    # Regex-only mode configuration
```

**What Works**:
- ‚úÖ Basic intent detection (keyword matching)
- ‚úÖ Simple entity extraction
- ‚úÖ No dependencies required
- ‚ö†Ô∏è  Lower accuracy than LLM mode

---

## ‚ùå Failure Scenarios & Recovery

### Problem 1: Ollama Installation Fails

**Symptoms**:
```
‚ùå Installation failed
```

**Recovery Options**:

1. **Manual Installation** (Recommended):
   ```bash
   # macOS
   brew install ollama

   # Linux
   curl -fsSL https://ollama.com/install.sh | sh

   # Windows
   # Download from https://ollama.com/download
   ```

2. **Re-run Installer**:
   ```bash
   ./install.sh
   ```

3. **Use Fallback Mode**:
   - Continue without Ollama
   - Installer will configure regex-only mode

### Problem 2: Server Won't Start

**Symptoms**:
```
‚ùå Failed to start Ollama server
```

**Troubleshooting**:

1. **Check if port 11434 is in use**:
   ```bash
   lsof -i :11434
   ```

2. **Kill conflicting process** (if safe):
   ```bash
   kill -9 <PID>
   ```

3. **Start server manually**:
   ```bash
   ollama serve
   ```

4. **Check logs**:
   ```bash
   # Linux
   journalctl -u ollama -f

   # macOS
   ~/Library/Logs/Ollama/server.log
   ```

### Problem 3: Model Download Fails

**Symptoms**:
```
‚ùå Failed to download model
```

**Recovery Options**:

1. **Retry Download**:
   ```bash
   ollama pull qwen2.5-coder:1.5b
   ```

2. **Check Network Connection**:
   ```bash
   ping ollama.com
   ```

3. **Try Smaller Model** (if RAM constrained):
   ```bash
   ollama pull qwen2.5-coder:0.5b
   ```

4. **Use Fallback Mode**:
   - Installer auto-configures regex-only mode

### Problem 4: Out of Memory During Pull

**Symptoms**:
- Model download stalls
- System becomes unresponsive
- OOM errors

**Recovery Options**:

1. **Increase Swap Space** (Linux):
   ```bash
   sudo fallocate -l 8G /swapfile
   sudo chmod 600 /swapfile
   sudo mkswap /swapfile
   sudo swapon /swapfile
   ```

2. **Try Smaller Model**:
   ```bash
   ollama pull qwen2.5-coder:0.5b  # ~512MB vs 1GB
   ```

3. **Close Other Applications**:
   - Free up RAM before installation

4. **Use Fallback Mode**:
   - No model download required

### Problem 5: Permission Denied

**Symptoms**:
```
‚ùå Permission denied
```

**Recovery Options**:

1. **Run with sudo** (Linux only):
   ```bash
   sudo ./install.sh
   ```

2. **Fix Directory Permissions**:
   ```bash
   chmod +x install.sh
   chmod +x install.js
   ```

3. **Check File Ownership**:
   ```bash
   ls -la .memories/
   ```

---

## üîÑ Re-running the Installer

The installer is **idempotent** - safe to run multiple times:

### If Already Installed (LLM Mode)

**What Happens**:
```
‚úÖ Ollama already installed: ollama version is 0.12.3
‚ÑπÔ∏è  Ollama server already running
‚úÖ Model already downloaded: qwen2.5-coder:1.5b
```

**Behavior**:
- Skips installation steps
- Verifies server is running
- Updates configuration (if needed)
- Completes in <10 seconds

### If Switching from Fallback to LLM Mode

**Scenario**: You initially declined Ollama, now want to enable it.

**Steps**:
1. **Run Installer Again**:
   ```bash
   ./install.sh
   ```

2. **Accept Ollama Installation**:
   ```
   Install Ollama now? (Y/n): y
   ```

3. **Wait for Completion**:
   - Ollama will install
   - Model will download
   - Config will update to `hybrid` mode

4. **Verify**:
   ```bash
   cat .memories/.llm-config.json
   # Should show "mode": "hybrid"
   ```

---

## üìÅ Configuration Files

### `.memories/.llm-config.json` (Hybrid Mode)

```json
{
  "mode": "hybrid",
  "llm": {
    "provider": "ollama",
    "model": "qwen2.5-coder:1.5b",
    "endpoint": "http://localhost:11434",
    "enabled": true,
    "fallback_to_regex": true
  },
  "regex": {
    "enabled": true,
    "use_as_fallback": true
  },
  "updated_at": "2025-10-05T12:34:56Z"
}
```

### `.memories/.llm-config.json` (Regex-Only Mode)

```json
{
  "mode": "regex-only",
  "llm": {
    "enabled": false,
    "reason": "Ollama not available or installation declined"
  },
  "regex": {
    "enabled": true,
    "patterns": [
      "intent-detection-basic",
      "entity-extraction-simple",
      "confidence-threshold-0.7"
    ]
  },
  "updated_at": "2025-10-05T12:34:56Z"
}
```

---

## üß™ Testing the Installation

### Test 1: Verify Ollama CLI

```bash
ollama --version
# Expected: "ollama version is 0.X.X"
```

### Test 2: Verify Ollama Server

```bash
curl http://localhost:11434/api/version
# Expected: {"version":"0.X.X"}
```

### Test 3: List Models

```bash
ollama list
# Expected: Should show qwen2.5-coder:1.5b
```

### Test 4: Test Model Inference

```bash
ollama run qwen2.5-coder:1.5b "Say hello"
# Expected: Model responds with greeting
```

### Test 5: Check Configuration

```bash
cat .memories/.llm-config.json | jq '.mode'
# Expected: "hybrid" or "regex-only"
```

---

## üîí Known Limitations

### Platform Limitations

1. **Windows Native**:
   - ‚ùå Bash installer not supported
   - ‚ö†Ô∏è  Node.js installer has limited functionality
   - ‚úÖ Must download Ollama manually from https://ollama.com/download

2. **WSL (Windows Subsystem for Linux)**:
   - ‚úÖ Bash installer works
   - ‚úÖ Node.js installer works
   - ‚ö†Ô∏è  May have performance overhead vs native Linux

3. **ARM Architecture**:
   - ‚úÖ macOS ARM64 (Apple Silicon) - Full support
   - ‚ö†Ô∏è  Linux ARM64 - Limited Ollama support

### Resource Limitations

1. **Low RAM Systems (<8GB)**:
   - ‚ö†Ô∏è  Model may not load
   - ‚ö†Ô∏è  System may swap excessively
   - ‚úÖ Fallback mode always works

2. **Low Disk Space (<12GB)**:
   - ‚ùå Cannot download model
   - ‚úÖ Can still install Ollama
   - ‚úÖ Fallback mode always works

3. **Slow Network**:
   - ‚ö†Ô∏è  Model download may timeout
   - ‚úÖ Can retry download manually

### Functional Limitations

1. **Offline Installation**:
   - ‚ùå Cannot download Ollama
   - ‚ùå Cannot download model
   - ‚úÖ Fallback mode works offline

2. **Proxy Networks**:
   - ‚ö†Ô∏è  May need proxy configuration
   - ‚ö†Ô∏è  HTTPS download may fail
   - ‚úÖ Fallback mode unaffected

3. **Corporate Firewalls**:
   - ‚ùå May block ollama.com
   - ‚ùå May block port 11434
   - ‚úÖ Fallback mode unaffected

---

## üõ†Ô∏è Manual Configuration

### Manually Configure LLM Mode

If you installed Ollama separately:

```bash
mkdir -p .memories

cat > .memories/.llm-config.json <<EOF
{
  "mode": "hybrid",
  "llm": {
    "provider": "ollama",
    "model": "qwen2.5-coder:1.5b",
    "endpoint": "http://localhost:11434",
    "enabled": true,
    "fallback_to_regex": true
  },
  "regex": {
    "enabled": true,
    "use_as_fallback": true
  },
  "updated_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
}
EOF
```

### Manually Configure Regex-Only Mode

```bash
mkdir -p .memories

cat > .memories/.llm-config.json <<EOF
{
  "mode": "regex-only",
  "llm": {
    "enabled": false,
    "reason": "Manual configuration"
  },
  "regex": {
    "enabled": true,
    "patterns": [
      "intent-detection-basic",
      "entity-extraction-simple",
      "confidence-threshold-0.7"
    ]
  },
  "updated_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
}
EOF
```

---

## üìû Support & Troubleshooting

### Getting Help

1. **Check Documentation**:
   - This guide (INSTALLER-GUIDE.md)
   - Design document (.memories/decisions/infrastructure/OLLAMA-INSTALLER-DESIGN.md)

2. **Check Logs**:
   ```bash
   # Ollama logs (Linux)
   journalctl -u ollama -f

   # Ollama logs (macOS)
   tail -f ~/Library/Logs/Ollama/server.log
   ```

3. **Verify Installation**:
   - Run all tests in "Testing the Installation" section

### Common Issues

See "Failure Scenarios & Recovery" section above.

### Emergency Fallback

If nothing works:

```bash
# Delete configuration
rm -f .memories/.llm-config.json

# Re-run installer
./install.sh

# When prompted, decline Ollama installation
# Installer will configure regex-only mode
```

---

## üìö Additional Resources

- **Ollama Documentation**: https://ollama.com/docs
- **Ollama GitHub**: https://github.com/ollama/ollama
- **Model Information**: https://ollama.com/library/qwen2.5-coder
- **Design Document**: `.memories/decisions/infrastructure/OLLAMA-INSTALLER-DESIGN.md`

---

**Version**: 1.0
**Last Updated**: October 5, 2025
**Installer Scripts**: `install.sh`, `install.js`
